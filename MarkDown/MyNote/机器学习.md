<style>
    a {text - decoration: none;}
    h1 {border - bottom: none; margin - top: auto;}
</style>


# <center>机器学习</center>

-  -  -

-  -  -

-  -  -

### 特征缩放

> ###### 目的

```java
提高机器学习算法的性能和稳定性, 确保各个特征在模型训练中起到合适的作用, 并且避免不同特征之间尺度差异引发的问题;
```

> ###### 常用方法

```java
标准化(Standardization): 减去特征均值并除以标准差, 将特征的值转换为均值为0, 标准差为1的分布;
```

$$
X^* = \frac{{X - \text{mean}(X)}}{{\text{std}(X)}}
$$

```java
归一化(Normalization): 减去特征最小值并除以特征最大值与以特征最小值的差值, 将特征值映射到0到1之间;
```

$$
X^* = \frac{{X - \min\left(X\right)}}{{\max\left(X\right) - \min\left(X\right)}}
$$

```java
固定范围缩放: 将特征缩放到自定义的范围, "a"和"b"是所需范围的最小值和最大值;
```

$$
X^* = a + (b - a) \cdot \frac{{X - \min\left(X\right)}}{{\max\left(X\right) - \min\left(X\right)}}
$$

> 注:

```java
为了保持模型的一致性和可靠性, 在对测试数据进行预测时, 应使用相同的训练数据的统计量(均值、标准差等)对测试数据进行同样的特征缩放操作;
```
---

### 线性回归

> ###### 定义

```java
线性回归是一种用于建立和预测变量之间线性关系的机器学习算法, 它是最简单和最常见的回归算法之一, 用于预测连续型目标变量;
```

> ###### 模型公式

$$
\hat{y^i} = f_{\vec{w},b}\left({\vec{x}}^i\right) = \vec{w}\cdot{\vec{x}}^i + b = w_1x_1^i + w_2x_2^i + w_3x_3^i + \cdots + w_nx_n^i + b = y^i + \varepsilon^i
$$

|      成员       |                             描述                             |
| :-------------: | :----------------------------------------------------------: |
|   $\hat{y^i}$   |                  样本$i$对应因变量的预测值                   |
|      $y^i$      |                  样本$i$对应因变量的真实值                   |
|  ${\vec{x}}^i$  |       样本$i$的特征向量$\{x_1^i,x_2^i,\cdots,x_n^i\}$        |
|  ${\vec{w}}^i$  |         特征的权重向量$\{w_1^i,w_2^i,\cdots,w_n^i\}$         |
|       $b$       |              常数项或偏置项, 与特征无关的偏移量              |
| $\varepsilon^i$ | 样本$i$的预测误差, 即模型预测值与真实值之间的差异<br/>$\varepsilon^i$在各样本间相互独立且服从均值为$0$方差为$\sigma^2$的高斯分布(正态分布) |

>###### 概率密度函数

$$
P\left(\varepsilon^i\right) =  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{\left(\varepsilon^i - \mu\right)^2}{2\sigma^2}\right) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{\left(\varepsilon^i\right)^2}{2\sigma^2}\right)
$$

由$\varepsilon^i = \hat{y^i} - y^i$得:
$$
P\left(y^i\vert{\vec{x}}^i;\vec{w},b\right) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( - \frac{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)^2}{2\sigma^2}\right)
$$

```java
上述边概率密度函数可以表示为在给定特征值、特征权重和偏置项得条件下, 样本i的模型预测值等于真实值的概率;
```

> ###### 似然函数

$$
L\left(\vec{w},b\right) = \prod_{i=1}^{m}{P\left(y^i\vert{\vec{x}}^i;\vec{w},b\right)} = \prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left( - \frac{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)^2}{2\sigma^2}\right)}
$$

```java
由于各样本之间是相互独立的, 所以线性回归的似然函数表示为各个样本概率密度函数的乘积;
```

> ###### 对数似然函数

$$
\ln{L\left(\vec{w},b\right)} = \ln{\prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left( - \frac{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)^2}{2\sigma^2}\right)}} = m\ln{\prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi\sigma^2}}}} - \frac{1}{2\sigma^2} \cdot \sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)^2}
$$

```java
取对数的目的是为了将乘法转换为加法进行求解, 通过使对数似然函数最大化, 求解权重向量w和偏置项b;
```

> ###### 损失函数

$$
J\left(\vec{w},b\right) = \frac{1}{2m}\sum_{i=1}^{m}{\left(\hat{y^i} - y^i\right)^2} = \frac{1}{2m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)^2}
$$

```java
损失(代价)函数中计算平方差的目的是放大较大的差异; 取均值的目的是对结果进行标准化, 使不同数据集之间的比较更加公平和一致; 除以2的目的是为了在梯度下降算法中简化计算, 用于求偏导时抵消常数系数; 
```

> ###### 求解目标

$$
\underset{\vec{w},b}{\max}L\left(\vec{w},b\right) \Rightarrow \underset{\vec{w},b}{\max}\ln{L\left(\vec{w},b\right)} \Rightarrow \underset{\vec{w},b}{\min}J\left(\vec{w},b\right)
$$

> ###### 梯度下降法求解

```java
梯度是指函数在某一点上的变化率或斜率, 由函数在每个自变量上的偏导数构成的向量表示; 梯度(偏导数)表示函数在给定点上指向函数增加最快的方向, 梯度下降的方向即梯度的反方向是指向函数减小最快的方向;
```

$$
\begin{align*}
     &\textbf{迭代$b$:} && b^* = b - \alpha\frac{\partial{J\left(\vec{w},b\right)}}{\partial b} = b - \alpha\frac{1}{m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)} \\[5pt]
     &\textbf{迭代$w_j$:} && w_j^* = w_j - \alpha\frac{\partial{J\left(\vec{w},b\right)}}{\partial w_j} = w_j - \alpha\frac{1}{m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)x_j^i}
\end{align*}
$$

```java
"ɑ"为学习率, 一个0到1之间的小数, 用于控制控制每次更新的步长, 可以理解为通过迭代使参数不断向梯度的反方向移动, 从而使损失函数的值不断接近极小值点;
```

```java
若"ɑ"过小, 易导致迭代时移动的距离过小, 致使迭代过程缓慢; 若"ɑ"过大, 易导致迭代时移动的距离过大, 跨过极小值点, 远离最优解, 致使损失函数的值不减反增;
```

```java
如果各特征的值范围差异很大, 梯度下降可能会在一个方向上收敛得很快,但在另一个方向上收敛缓慢; 通过特征缩放可以使目标函数的等高线更接近圆形, 使梯度下降在各个方向上更均匀地进行, 从而提高算法的收敛速度;
```

```java
当结果达到算法迭代的终止条件(达到最大迭代次数、目标函数收敛等)时, 求得的权重向量w和偏置项b为最优解; 值得注意的是, 意梯度下降算法可能会陷入局部最优解, 因此多次运行算法，每次使用不同的初始值，可以增加找到最优解的机会;
```

---

### 逻辑回归

> ###### 定义

```java
逻辑回归是一种常用的机器学习算法, 用于解决二分类(分别用"0"和"1"表示)问题, 其目标是根据输入特征的线性组合来预测样本属于某个类别(由"1"表示)的概率;
```

> ###### 模型公式

$$
f_{\vec{w},b}\left({\vec{x}}^i\right) = g\left(z\right) = g\left(\vec{w}\cdot{\vec{x}}^i + b\right) = \frac{1}{1 + e^{-z}} = \frac{1}{1 + e^{-\left(\vec{w}\cdot{\vec{x}}^i + b\right)}} = \frac{1}{1 + e^{-\left(w_1x_1^i + w_2x_2^i + w_3x_3^i + \cdots + b\right)}}
$$

|     成员      |                      描述                       |
| :-----------: | :---------------------------------------------: |
|      $z$      |               特征对应的线性组合                |
|     $y^i$     |          样本$i$对应的真实类型$0$或$1$          |
|  $\hat{y^i}$  |          样本$i$对应的预测类型$0$或$1$          |
| ${\vec{x}}^i$ | 样本$i$的特征向量$\{x_1^i,x_2^i,\cdots,x_n^i\}$ |
| ${\vec{w}}^i$ |  特征的权重向量$\{w_1^i,w_2^i,\cdots,w_n^i\}$   |
|      $b$      |       常数项或偏置项, 与特征无关的偏移量        |

> ###### 概率密度函数

$$
\begin{align}
& \textbf{由:} \quad P\left(\hat{y^i}=1\vert{\vec{x}}^i;\vec{w},b\right) = f_{\vec{w},b}\left({\vec{x}}^i\right); \quad P\left(\hat{y^i}=0\vert{\vec{x}}^i;\vec{w},b\right) = 1 - f_{\vec{w},b}\left({\vec{x}}^i\right) \\[5pt]
& \textbf{得:} \quad P\left(y^i\vert{\vec{x}}^i;\vec{w},b\right) = P\left(\hat{y^i}=1\vert{\vec{x}}^i;\vec{w},b\right)^{y^i} \cdot P\left(\hat{y^i}=0\vert{\vec{x}}^i;\vec{w},b\right)^{1-y^i} = \left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^{y^i} \cdot \left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^{1-y^i}
\end{align}
$$

```java
上述边概率密度函数可以表示为在给定特征值、特征权重和偏置项得条件下, 样本i的模型预测类型等于真实类型的概率;
```

> ###### 似然函数

$$
L\left(\vec{w},b\right) = \prod_{i=1}^{m}{P\left(y^i\vert{\vec{x}}^i;\vec{w},b\right)} = \prod_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^{y^i} \cdot \left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^{1-y^i}}
$$

```java
由于各样本之间是相互独立的, 所以逻辑回归的似然函数表示为各个样本概率密度函数的乘积;
```

> ###### 对数似然函数

$$
\ln{L\left(\vec{w},b\right)} = \ln{\prod_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^{y^i} \cdot \left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^{1-y^i}}} = \sum_{i=1}^{m}{\left(y^i \cdot \ln\left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right) + (1 - y^i) \cdot \ln\left(1 - f_{\vec{w},b}\left({\vec{x}}^i\right)\right)\right)}
$$

```java
取对数的目的是为了将乘法转换为加法进行求解, 通过使对数似然函数最大化, 求解权重向量w和偏置项b;
```

> ###### 损失函数

$$
\begin{align}
    & \textbf{样本损失函数:} && L\left(f_{\vec{w},b}\left({\vec{x}}^i\right),y^i\right) = 
    \begin{cases}
        -\ln\left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right) & if & y^i=1 \\[5pt]
        -\ln\left(1 - f_{\vec{w},b}\left({\vec{x}}^i\right)\right) & if & y^i=0 
    \end{cases} \\[5pt]
    & \textbf{整体损失函数:} && J\left(\vec{w},b\right) = \frac{1}{m}\sum_{i=1}^{m}{L\left(f_{\vec{w},b}\left({\vec{x}}^i\right),y^i\right)} = -\frac{1}{m}\sum_{i=1}^{m}{\left(y^i \cdot \ln\left(f_{\vec{w},b}\left({\vec{x}}^i\right)\right) + (1 - y^i) \cdot \ln\left(1 - f_{\vec{w},b}\left({\vec{x}}^i\right)\right)\right)}
\end{align}
$$

```java
对于每个样本, 当真实标签为"0"时, 我们期望其预测的概率越小损失越小; 当真实标签为"1"时, 我们期望其预测的概率越大损失越小;
```

> ###### 求解目标

$$
\underset{\vec{w},b}{\max}L\left(\vec{w},b\right) \Rightarrow \underset{\vec{w},b}{\max}\ln{L\left(\vec{w},b\right)} \Rightarrow \underset{\vec{w},b}{\min}J\left(\vec{w},b\right)
$$

> ###### 梯度下降法求解

```java
梯度是指函数在某一点上的变化率或斜率, 由函数在每个自变量上的偏导数构成的向量表示; 梯度(偏导数)表示函数在给定点上指向函数增加最快的方向, 梯度下降的方向即梯度的反方向是指向函数减小最快的方向;
```

$$
\begin{align*}
     &\textbf{迭代$b$:} && b^* = b - \alpha\frac{\partial{J\left(\vec{w},b\right)}}{\partial b} = b - \alpha\frac{1}{m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)} \\[5pt]
     &\textbf{迭代$w_j$:} && w_j^* = w_j - \alpha\frac{\partial{J\left(\vec{w},b\right)}}{\partial w_j} = w_j - \alpha\frac{1}{m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right) - y^i\right)x_j^i}
\end{align*}
$$

```java
"ɑ"为学习率, 一个0到1之间的小数, 用于控制控制每次更新的步长, 可以理解为通过迭代使参数不断向梯度的反方向移动, 从而使损失函数的值不断接近极小值点;
```

```java
若"ɑ"过小, 易导致迭代时移动的距离过小, 致使迭代过程缓慢; 若"ɑ"过大, 易导致迭代时移动的距离过大, 跨过极小值点, 远离最优解, 致使损失函数的值不减反增;
```

```java
如果各特征的值范围差异很大, 梯度下降可能会在一个方向上收敛得很快,但在另一个方向上收敛缓慢; 通过特征缩放可以使目标函数的等高线更接近圆形, 使梯度下降在各个方向上更均匀地进行, 从而提高算法的收敛速度;
```

```java
当结果达到算法迭代的终止条件(达到最大迭代次数、目标函数收敛等)时, 求得的权重向量w和偏置项b为最优解; 值得注意的是, 意梯度下降算法可能会陷入局部最优解, 因此多次运行算法，每次使用不同的初始值，可以增加找到最优解的机会;
```

---

### 模型过拟合

> ###### 定义

```java
过拟合是指机器学习模型在训练数据上表现出很好的性能, 但在新的、未见过的数据上表现较差的现象; 当模型过拟合时, 它过于复杂地学习了训练数据中的噪声或随机变动, 而无法很好地泛化到新的数据集上;
```

> ###### 解决方案

```java
1.数据集扩充: 通过扩大训练集样本量, 增强样本的多样性;
2.模型复杂度调整: 通过特征选择, 减少参数数量, 降低模型的复杂度;
3.正则化: 通过对模型的复杂度进行惩罚, 限制模型参数的取值范围, 抑制参数过大的影响;
```

> ###### L2正则化损失函数

$$
\begin{align*}
     &\textbf{正则化惩罚项:} && \lambda\sum_{i=1}^{n}{w_i^2} \\[5pt]
     &\textbf{正则化损失函数:} && {J\left(\vec{w},b\right)}^* = J\left(\vec{w},b\right) + \lambda\sum_{i=1}^{n}{w_i^2}
\end{align*}
$$

```java
"λ"是正则化参数, 决定了正则化的强度;
```

> ###### 求解目标

$$
\underset{\vec{w},b}{\min}J\left(\vec{w},b\right) \Rightarrow \underset{\vec{w},b}{\min}{J\left(\vec{w},b\right)}^*
$$



> ###### 梯度下降法迭代

$$
\begin{align*}
     &\textbf{迭代$b$:} && b^* = b - \alpha\frac{\partial{{J\left(\vec{w},b\right)}^*}}{\partial b} = b - \alpha\frac{\partial{J\left(\vec{w},b\right)}}{\partial b} \\[5pt]
     &\textbf{迭代$w_j$:} && w_j^* = w_j - \alpha\frac{\partial{{J\left(\vec{w},b\right)}^*}}{\partial w_j} = w_j - \alpha\left(\frac{\partial{J\left(\vec{w},b\right)}}{\partial w_j} + 2\lambda{w_j}\right)
\end{align*}
$$

```java
较大的λ值会增强正则化的效果, 使模型的权重趋向于较小的值; 较小的λ值会减弱正则化的效果; 允许模型权重取较大的值; 如果λ值过大会, 强制模型的权重趋向于0, 会限制模型的复杂度, 使模型变得过于简单, 无法充分拟合训练数据, 导致欠拟合现象;
```

---

### 神经元网络

> ###### 定义

```java
神经元网络, 是一种模拟人脑神经系统的计算模型; 它由大量相互连接的基本处理单元(神经元)组成, 通过这些神经元之间的连接和信息传递来实现复杂的计算任务; 神经元网络通常由多个层次组成, 包括输入层、隐藏层和输出层, 输入信号在隐藏层中经过每一层的神经元计算, 通过加权求和或非线性激活函数产生新的特征, 这些新的特征被传递到下一层作为输入, 进一步进行计算和转换, 直到最后由输出层生成最终的计算结果;
```

> ###### 图形展示

![NeuralNetwork](https://hmu-wh.github.io/MyNote/MachineLearning/Image/NeuralNetwork.png)
