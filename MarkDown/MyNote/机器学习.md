<style>
    a {text-decoration: none;}
    h1 {border-bottom: none; margin-top: auto;}
</style>
# <center>机器学习</center>

---

---

---

### 线性回归

> ###### 模型公式

$$
\hat{y^i} = f_{\vec{w},b}\left({\vec{x}}^i\right) = \vec{w}\cdot{\vec{x}}^i+b = w_1x_1^i+w_2x_2^i+w_3x_3^i+···+b = y^i-\varepsilon^i
$$



|      成员       |                             描述                             |
| :-------------: | :----------------------------------------------------------: |
|   $\hat{y^i}$   |                 样本$i$的对应因变量的预测值                  |
|      $y^i$      |                 样本$i$的对应因变量的真实值                  |
|  ${\vec{x}}^i$  |           样本$i$的特征向量$\{x_1^i, x_2^i, ···\}$           |
|    $\vec{w}$    |            特征对应的权重向量$\{w_1, w_2, ···\}$             |
|       $b$       |             常数项或偏置项, 与自变量无关的偏移量             |
| $\varepsilon^i$ | 样本$i$的预测误差, 即模型预测值与真实值之间的差异<br>$\varepsilon^i$在各样本间相互独立且服从均值为$0$方差为$\sigma^2$的高斯分布(正态分布) |

>###### 边缘概率

$$
P\left(\varepsilon^i\right) =  \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\varepsilon^i-\mu\right)^2}{2\sigma^2}\right) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(\varepsilon^i\right)^2}{2\sigma^2}\right)
$$

由$\varepsilon^i = y^i-\hat{y^i}$得:
$$
P\left(y^i|{\vec{x}}^i,\vec{w},b\right) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{\left(y^i-f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^2}{2\sigma^2}\right)
$$

```python
上述边缘概率公式可以表示为在给定特征值、特征权重和偏置项得条件下, 样本i的模型预测值等于真实值的概率;
```

> ###### 似然函数(联合概率)

$$
L\left(\vec{w},b\right) = \prod_{i=1}^{m}{P\left(y^i|{\vec{x}}^i,\vec{w},b\right)} = \prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{\left(y^i-f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^2}{2\sigma^2}\right)}
$$

```python
由于各样本之间是相互独立的, 所以线性回归的联合概率表示为各个样本边缘概率的乘积
```

> ###### 对数似然函数

$$
\ln{L\left(\vec{w},b\right)} = \ln{\prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{\left(y^i-f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^2}{2\sigma^2}\right)}} = m\ln{\prod_{i=1}^{m}{\frac{1}{\sqrt{2\pi\sigma^2}}}}-\frac{1}{2\sigma^2}\cdot\sum_{i=1}^{m}{\left(y^i-f_{\vec{w},b}\left({\vec{x}}^i\right)\right)^2}
$$

```python
取对数的目的是为了将乘法转换为加法进行求解, 通过使对数似然函数最大化, 求解权重向量w和偏置项b;
```

> ###### 损失函数(均方误差法)

$$
J\left(\vec{w},b\right) = \frac{1}{2m}\sum_{i=1}^{m}{\left(\hat{y^i}-y^i\right)^2} = \frac{1}{2m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right)-y^i\right)^2}
$$

```python
损失(代价)函数中计算平方差的目的是放大较大的差异; 取均值的目的是对结果进行标准化, 使不同数据集之间的比较更加公平和一致; 除以2的目的是为了在梯度下降算法中简化计算, 用于求偏导时抵消常数系数; 
```

> ###### 求解目标

$$
\underset{\vec{w},b}{\max}L\left(\vec{w},b\right)\Rightarrow\underset{\theta}{\max}\ln{L\left(\vec{w},b\right)}\Rightarrow\underset{\vec{w},b}{\min}J\left(\vec{w},b\right)
$$

> ###### 梯度下降法求解

```python
梯度(偏导数)表示函数在给定点上指向函数增加最快的方向, 梯度的反方向指向函数减小最快的方向;
```

**对于$w_j$ 和$b$, 将采用如下方式进行迭代:**
$$
\begin{align*}
& b^* = b-\alpha\frac{\partial{J\left(\vec{w},b\right)}}{\partial b} = b-\alpha\frac{1}{m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right)-y^i\right)} \\[5pt]
& w_j^* = w_j-\alpha\frac{\partial{J\left(\vec{w},b\right)}}{\partial w_j} = w_j-\alpha\frac{1}{m}\sum_{i=1}^{m}{\left(f_{\vec{w},b}\left({\vec{x}}^i\right)-y^i\right)x_j^i}
\end{align*}
$$

```python
"ɑ"为学习率, 一个0到1之间的小数, 用于控制控制每次更新的步长, 可以理解为通过迭代使参数不断向梯度的反方向移动, 从而使损失函数的值不断接近极小值点;
```

```python
若"ɑ"过小, 易导致迭代时移动的距离过小, 致使迭代过程缓慢; 若"ɑ"过大, 易导致迭代时移动的距离过大, 跨过极小值点, 致使远离最优解;
```

```python
当结果达到算法迭代的终止条件(达到最大迭代次数、目标函数收敛等)时, 求得的权重向量w和偏置项b为最优解; 值得注意的是, 意梯度下降算法可能会陷入局部最优解, 因此多次运行算法，每次使用不同的初始值，可以增加找到最优解的机会;
```















 



